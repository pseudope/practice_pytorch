{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)   # channel, output, kernel, stride\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(12 * 12* 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if b_i % 10 == 0:\n",
    "            print(\"Epoch: {} [{}/{} ({:.0f}%)] \\t Train Loss: {:.6f}\"\n",
    "            .format(epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "            100. * (b_i / len(train_dataloader)), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss, success = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction=\"sum\").item()\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "    print(\"\\nTest Dataset\")\n",
    "    print(\"Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\"\n",
    "    .format(loss, success, len(test_dataloader.dataset), 100. * (success / len(test_dataloader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets.MNIST(\"../data\", train=True, download=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1302,), (0.3069))])),batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(datasets.MNIST(\"../data\", train=False, download=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1302,), (0.3069))])),batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0 / 60000 (0%)] \t Train Loss: 0.065752\n",
      "Epoch: 1 [320 / 60000 (1%)] \t Train Loss: 0.046321\n",
      "Epoch: 1 [640 / 60000 (1%)] \t Train Loss: 0.182308\n",
      "Epoch: 1 [960 / 60000 (2%)] \t Train Loss: 0.100019\n",
      "Epoch: 1 [1280 / 60000 (2%)] \t Train Loss: 0.046793\n",
      "Epoch: 1 [1600 / 60000 (3%)] \t Train Loss: 0.004141\n",
      "Epoch: 1 [1920 / 60000 (3%)] \t Train Loss: 0.088554\n",
      "Epoch: 1 [2240 / 60000 (4%)] \t Train Loss: 0.027893\n",
      "Epoch: 1 [2560 / 60000 (4%)] \t Train Loss: 0.172537\n",
      "Epoch: 1 [2880 / 60000 (5%)] \t Train Loss: 0.014431\n",
      "Epoch: 1 [3200 / 60000 (5%)] \t Train Loss: 0.053399\n",
      "Epoch: 1 [3520 / 60000 (6%)] \t Train Loss: 0.008097\n",
      "Epoch: 1 [3840 / 60000 (6%)] \t Train Loss: 0.085096\n",
      "Epoch: 1 [4160 / 60000 (7%)] \t Train Loss: 0.018852\n",
      "Epoch: 1 [4480 / 60000 (7%)] \t Train Loss: 0.032551\n",
      "Epoch: 1 [4800 / 60000 (8%)] \t Train Loss: 0.041550\n",
      "Epoch: 1 [5120 / 60000 (9%)] \t Train Loss: 0.176699\n",
      "Epoch: 1 [5440 / 60000 (9%)] \t Train Loss: 0.147903\n",
      "Epoch: 1 [5760 / 60000 (10%)] \t Train Loss: 0.092533\n",
      "Epoch: 1 [6080 / 60000 (10%)] \t Train Loss: 0.067616\n",
      "Epoch: 1 [6400 / 60000 (11%)] \t Train Loss: 0.039855\n",
      "Epoch: 1 [6720 / 60000 (11%)] \t Train Loss: 0.037211\n",
      "Epoch: 1 [7040 / 60000 (12%)] \t Train Loss: 0.011555\n",
      "Epoch: 1 [7360 / 60000 (12%)] \t Train Loss: 0.052929\n",
      "Epoch: 1 [7680 / 60000 (13%)] \t Train Loss: 0.019826\n",
      "Epoch: 1 [8000 / 60000 (13%)] \t Train Loss: 0.254084\n",
      "Epoch: 1 [8320 / 60000 (14%)] \t Train Loss: 0.035964\n",
      "Epoch: 1 [8640 / 60000 (14%)] \t Train Loss: 0.018464\n",
      "Epoch: 1 [8960 / 60000 (15%)] \t Train Loss: 0.011549\n",
      "Epoch: 1 [9280 / 60000 (15%)] \t Train Loss: 0.003519\n",
      "Epoch: 1 [9600 / 60000 (16%)] \t Train Loss: 0.053559\n",
      "Epoch: 1 [9920 / 60000 (17%)] \t Train Loss: 0.005516\n",
      "Epoch: 1 [10240 / 60000 (17%)] \t Train Loss: 0.014382\n",
      "Epoch: 1 [10560 / 60000 (18%)] \t Train Loss: 0.000905\n",
      "Epoch: 1 [10880 / 60000 (18%)] \t Train Loss: 0.069279\n",
      "Epoch: 1 [11200 / 60000 (19%)] \t Train Loss: 0.016699\n",
      "Epoch: 1 [11520 / 60000 (19%)] \t Train Loss: 0.007585\n",
      "Epoch: 1 [11840 / 60000 (20%)] \t Train Loss: 0.103950\n",
      "Epoch: 1 [12160 / 60000 (20%)] \t Train Loss: 0.040103\n",
      "Epoch: 1 [12480 / 60000 (21%)] \t Train Loss: 0.053891\n",
      "Epoch: 1 [12800 / 60000 (21%)] \t Train Loss: 0.005434\n",
      "Epoch: 1 [13120 / 60000 (22%)] \t Train Loss: 0.156693\n",
      "Epoch: 1 [13440 / 60000 (22%)] \t Train Loss: 0.042166\n",
      "Epoch: 1 [13760 / 60000 (23%)] \t Train Loss: 0.106151\n",
      "Epoch: 1 [14080 / 60000 (23%)] \t Train Loss: 0.061362\n",
      "Epoch: 1 [14400 / 60000 (24%)] \t Train Loss: 0.022028\n",
      "Epoch: 1 [14720 / 60000 (25%)] \t Train Loss: 0.010075\n",
      "Epoch: 1 [15040 / 60000 (25%)] \t Train Loss: 0.001469\n",
      "Epoch: 1 [15360 / 60000 (26%)] \t Train Loss: 0.090756\n",
      "Epoch: 1 [15680 / 60000 (26%)] \t Train Loss: 0.062046\n",
      "Epoch: 1 [16000 / 60000 (27%)] \t Train Loss: 0.092703\n",
      "Epoch: 1 [16320 / 60000 (27%)] \t Train Loss: 0.155781\n",
      "Epoch: 1 [16640 / 60000 (28%)] \t Train Loss: 0.006965\n",
      "Epoch: 1 [16960 / 60000 (28%)] \t Train Loss: 0.352575\n",
      "Epoch: 1 [17280 / 60000 (29%)] \t Train Loss: 0.046059\n",
      "Epoch: 1 [17600 / 60000 (29%)] \t Train Loss: 0.051535\n",
      "Epoch: 1 [17920 / 60000 (30%)] \t Train Loss: 0.104968\n",
      "Epoch: 1 [18240 / 60000 (30%)] \t Train Loss: 0.085395\n",
      "Epoch: 1 [18560 / 60000 (31%)] \t Train Loss: 0.001344\n",
      "Epoch: 1 [18880 / 60000 (31%)] \t Train Loss: 0.058310\n",
      "Epoch: 1 [19200 / 60000 (32%)] \t Train Loss: 0.176606\n",
      "Epoch: 1 [19520 / 60000 (33%)] \t Train Loss: 0.007442\n",
      "Epoch: 1 [19840 / 60000 (33%)] \t Train Loss: 0.086141\n",
      "Epoch: 1 [20160 / 60000 (34%)] \t Train Loss: 0.020144\n",
      "Epoch: 1 [20480 / 60000 (34%)] \t Train Loss: 0.132125\n",
      "Epoch: 1 [20800 / 60000 (35%)] \t Train Loss: 0.012990\n",
      "Epoch: 1 [21120 / 60000 (35%)] \t Train Loss: 0.051825\n",
      "Epoch: 1 [21440 / 60000 (36%)] \t Train Loss: 0.030699\n",
      "Epoch: 1 [21760 / 60000 (36%)] \t Train Loss: 0.013461\n",
      "Epoch: 1 [22080 / 60000 (37%)] \t Train Loss: 0.008014\n",
      "Epoch: 1 [22400 / 60000 (37%)] \t Train Loss: 0.002708\n",
      "Epoch: 1 [22720 / 60000 (38%)] \t Train Loss: 0.008574\n",
      "Epoch: 1 [23040 / 60000 (38%)] \t Train Loss: 0.049764\n",
      "Epoch: 1 [23360 / 60000 (39%)] \t Train Loss: 0.121698\n",
      "Epoch: 1 [23680 / 60000 (39%)] \t Train Loss: 0.153834\n",
      "Epoch: 1 [24000 / 60000 (40%)] \t Train Loss: 0.038332\n",
      "Epoch: 1 [24320 / 60000 (41%)] \t Train Loss: 0.022061\n",
      "Epoch: 1 [24640 / 60000 (41%)] \t Train Loss: 0.121854\n",
      "Epoch: 1 [24960 / 60000 (42%)] \t Train Loss: 0.054326\n",
      "Epoch: 1 [25280 / 60000 (42%)] \t Train Loss: 0.047902\n",
      "Epoch: 1 [25600 / 60000 (43%)] \t Train Loss: 0.074178\n",
      "Epoch: 1 [25920 / 60000 (43%)] \t Train Loss: 0.020641\n",
      "Epoch: 1 [26240 / 60000 (44%)] \t Train Loss: 0.002279\n",
      "Epoch: 1 [26560 / 60000 (44%)] \t Train Loss: 0.007554\n",
      "Epoch: 1 [26880 / 60000 (45%)] \t Train Loss: 0.073031\n",
      "Epoch: 1 [27200 / 60000 (45%)] \t Train Loss: 0.451457\n",
      "Epoch: 1 [27520 / 60000 (46%)] \t Train Loss: 0.096736\n",
      "Epoch: 1 [27840 / 60000 (46%)] \t Train Loss: 0.009604\n",
      "Epoch: 1 [28160 / 60000 (47%)] \t Train Loss: 0.031672\n",
      "Epoch: 1 [28480 / 60000 (47%)] \t Train Loss: 0.118580\n",
      "Epoch: 1 [28800 / 60000 (48%)] \t Train Loss: 0.008596\n",
      "Epoch: 1 [29120 / 60000 (49%)] \t Train Loss: 0.003814\n",
      "Epoch: 1 [29440 / 60000 (49%)] \t Train Loss: 0.041830\n",
      "Epoch: 1 [29760 / 60000 (50%)] \t Train Loss: 0.052880\n",
      "Epoch: 1 [30080 / 60000 (50%)] \t Train Loss: 0.018723\n",
      "Epoch: 1 [30400 / 60000 (51%)] \t Train Loss: 0.024470\n",
      "Epoch: 1 [30720 / 60000 (51%)] \t Train Loss: 0.052251\n",
      "Epoch: 1 [31040 / 60000 (52%)] \t Train Loss: 0.185668\n",
      "Epoch: 1 [31360 / 60000 (52%)] \t Train Loss: 0.071440\n",
      "Epoch: 1 [31680 / 60000 (53%)] \t Train Loss: 0.016007\n",
      "Epoch: 1 [32000 / 60000 (53%)] \t Train Loss: 0.007070\n",
      "Epoch: 1 [32320 / 60000 (54%)] \t Train Loss: 0.004238\n",
      "Epoch: 1 [32640 / 60000 (54%)] \t Train Loss: 0.016457\n",
      "Epoch: 1 [32960 / 60000 (55%)] \t Train Loss: 0.001081\n",
      "Epoch: 1 [33280 / 60000 (55%)] \t Train Loss: 0.114469\n",
      "Epoch: 1 [33600 / 60000 (56%)] \t Train Loss: 0.015104\n",
      "Epoch: 1 [33920 / 60000 (57%)] \t Train Loss: 0.009757\n",
      "Epoch: 1 [34240 / 60000 (57%)] \t Train Loss: 0.282352\n",
      "Epoch: 1 [34560 / 60000 (58%)] \t Train Loss: 0.005977\n",
      "Epoch: 1 [34880 / 60000 (58%)] \t Train Loss: 0.107109\n",
      "Epoch: 1 [35200 / 60000 (59%)] \t Train Loss: 0.020381\n",
      "Epoch: 1 [35520 / 60000 (59%)] \t Train Loss: 0.047946\n",
      "Epoch: 1 [35840 / 60000 (60%)] \t Train Loss: 0.003976\n",
      "Epoch: 1 [36160 / 60000 (60%)] \t Train Loss: 0.019690\n",
      "Epoch: 1 [36480 / 60000 (61%)] \t Train Loss: 0.085726\n",
      "Epoch: 1 [36800 / 60000 (61%)] \t Train Loss: 0.001582\n",
      "Epoch: 1 [37120 / 60000 (62%)] \t Train Loss: 0.537522\n",
      "Epoch: 1 [37440 / 60000 (62%)] \t Train Loss: 0.004782\n",
      "Epoch: 1 [37760 / 60000 (63%)] \t Train Loss: 0.074927\n",
      "Epoch: 1 [38080 / 60000 (63%)] \t Train Loss: 0.010549\n",
      "Epoch: 1 [38400 / 60000 (64%)] \t Train Loss: 0.027789\n",
      "Epoch: 1 [38720 / 60000 (65%)] \t Train Loss: 0.084818\n",
      "Epoch: 1 [39040 / 60000 (65%)] \t Train Loss: 0.008552\n",
      "Epoch: 1 [39360 / 60000 (66%)] \t Train Loss: 0.003522\n",
      "Epoch: 1 [39680 / 60000 (66%)] \t Train Loss: 0.188289\n",
      "Epoch: 1 [40000 / 60000 (67%)] \t Train Loss: 0.007292\n",
      "Epoch: 1 [40320 / 60000 (67%)] \t Train Loss: 0.270734\n",
      "Epoch: 1 [40640 / 60000 (68%)] \t Train Loss: 0.517777\n",
      "Epoch: 1 [40960 / 60000 (68%)] \t Train Loss: 0.061498\n",
      "Epoch: 1 [41280 / 60000 (69%)] \t Train Loss: 0.078987\n",
      "Epoch: 1 [41600 / 60000 (69%)] \t Train Loss: 0.173754\n",
      "Epoch: 1 [41920 / 60000 (70%)] \t Train Loss: 0.006527\n",
      "Epoch: 1 [42240 / 60000 (70%)] \t Train Loss: 0.018563\n",
      "Epoch: 1 [42560 / 60000 (71%)] \t Train Loss: 0.046074\n",
      "Epoch: 1 [42880 / 60000 (71%)] \t Train Loss: 0.007381\n",
      "Epoch: 1 [43200 / 60000 (72%)] \t Train Loss: 0.002868\n",
      "Epoch: 1 [43520 / 60000 (73%)] \t Train Loss: 0.031981\n",
      "Epoch: 1 [43840 / 60000 (73%)] \t Train Loss: 0.229525\n",
      "Epoch: 1 [44160 / 60000 (74%)] \t Train Loss: 0.197965\n",
      "Epoch: 1 [44480 / 60000 (74%)] \t Train Loss: 0.008334\n",
      "Epoch: 1 [44800 / 60000 (75%)] \t Train Loss: 0.004154\n",
      "Epoch: 1 [45120 / 60000 (75%)] \t Train Loss: 0.071964\n",
      "Epoch: 1 [45440 / 60000 (76%)] \t Train Loss: 0.036430\n",
      "Epoch: 1 [45760 / 60000 (76%)] \t Train Loss: 0.033906\n",
      "Epoch: 1 [46080 / 60000 (77%)] \t Train Loss: 0.060609\n",
      "Epoch: 1 [46400 / 60000 (77%)] \t Train Loss: 0.021536\n",
      "Epoch: 1 [46720 / 60000 (78%)] \t Train Loss: 0.106873\n",
      "Epoch: 1 [47040 / 60000 (78%)] \t Train Loss: 0.027134\n",
      "Epoch: 1 [47360 / 60000 (79%)] \t Train Loss: 0.036976\n",
      "Epoch: 1 [47680 / 60000 (79%)] \t Train Loss: 0.023111\n",
      "Epoch: 1 [48000 / 60000 (80%)] \t Train Loss: 0.101511\n",
      "Epoch: 1 [48320 / 60000 (81%)] \t Train Loss: 0.213937\n",
      "Epoch: 1 [48640 / 60000 (81%)] \t Train Loss: 0.038492\n",
      "Epoch: 1 [48960 / 60000 (82%)] \t Train Loss: 0.005773\n",
      "Epoch: 1 [49280 / 60000 (82%)] \t Train Loss: 0.002885\n",
      "Epoch: 1 [49600 / 60000 (83%)] \t Train Loss: 0.006899\n",
      "Epoch: 1 [49920 / 60000 (83%)] \t Train Loss: 0.087912\n",
      "Epoch: 1 [50240 / 60000 (84%)] \t Train Loss: 0.034964\n",
      "Epoch: 1 [50560 / 60000 (84%)] \t Train Loss: 0.003793\n",
      "Epoch: 1 [50880 / 60000 (85%)] \t Train Loss: 0.004545\n",
      "Epoch: 1 [51200 / 60000 (85%)] \t Train Loss: 0.016994\n",
      "Epoch: 1 [51520 / 60000 (86%)] \t Train Loss: 0.211518\n",
      "Epoch: 1 [51840 / 60000 (86%)] \t Train Loss: 0.005111\n",
      "Epoch: 1 [52160 / 60000 (87%)] \t Train Loss: 0.004229\n",
      "Epoch: 1 [52480 / 60000 (87%)] \t Train Loss: 0.141096\n",
      "Epoch: 1 [52800 / 60000 (88%)] \t Train Loss: 0.030710\n",
      "Epoch: 1 [53120 / 60000 (89%)] \t Train Loss: 0.004595\n",
      "Epoch: 1 [53440 / 60000 (89%)] \t Train Loss: 0.145030\n",
      "Epoch: 1 [53760 / 60000 (90%)] \t Train Loss: 0.164659\n",
      "Epoch: 1 [54080 / 60000 (90%)] \t Train Loss: 0.004534\n",
      "Epoch: 1 [54400 / 60000 (91%)] \t Train Loss: 0.397145\n",
      "Epoch: 1 [54720 / 60000 (91%)] \t Train Loss: 0.127476\n",
      "Epoch: 1 [55040 / 60000 (92%)] \t Train Loss: 0.026379\n",
      "Epoch: 1 [55360 / 60000 (92%)] \t Train Loss: 0.073533\n",
      "Epoch: 1 [55680 / 60000 (93%)] \t Train Loss: 0.006717\n",
      "Epoch: 1 [56000 / 60000 (93%)] \t Train Loss: 0.026104\n",
      "Epoch: 1 [56320 / 60000 (94%)] \t Train Loss: 0.001623\n",
      "Epoch: 1 [56640 / 60000 (94%)] \t Train Loss: 0.083629\n",
      "Epoch: 1 [56960 / 60000 (95%)] \t Train Loss: 0.044816\n",
      "Epoch: 1 [57280 / 60000 (95%)] \t Train Loss: 0.023080\n",
      "Epoch: 1 [57600 / 60000 (96%)] \t Train Loss: 0.073662\n",
      "Epoch: 1 [57920 / 60000 (97%)] \t Train Loss: 0.140032\n",
      "Epoch: 1 [58240 / 60000 (97%)] \t Train Loss: 0.057003\n",
      "Epoch: 1 [58560 / 60000 (98%)] \t Train Loss: 0.006074\n",
      "Epoch: 1 [58880 / 60000 (98%)] \t Train Loss: 0.004899\n",
      "Epoch: 1 [59200 / 60000 (99%)] \t Train Loss: 0.034575\n",
      "Epoch: 1 [59520 / 60000 (99%)] \t Train Loss: 0.005412\n",
      "Epoch: 1 [59840 / 60000 (100%)] \t Train Loss: 0.011262\n",
      "\n",
      "Test Dataset\n",
      "Loss: 0.0426, Accuracy: 9866/10000 (99%)\n",
      "Epoch: 2 [0 / 60000 (0%)] \t Train Loss: 0.006810\n",
      "Epoch: 2 [320 / 60000 (1%)] \t Train Loss: 0.033606\n",
      "Epoch: 2 [640 / 60000 (1%)] \t Train Loss: 0.052454\n",
      "Epoch: 2 [960 / 60000 (2%)] \t Train Loss: 0.006332\n",
      "Epoch: 2 [1280 / 60000 (2%)] \t Train Loss: 0.032838\n",
      "Epoch: 2 [1600 / 60000 (3%)] \t Train Loss: 0.011189\n",
      "Epoch: 2 [1920 / 60000 (3%)] \t Train Loss: 0.029846\n",
      "Epoch: 2 [2240 / 60000 (4%)] \t Train Loss: 0.054123\n",
      "Epoch: 2 [2560 / 60000 (4%)] \t Train Loss: 0.070998\n",
      "Epoch: 2 [2880 / 60000 (5%)] \t Train Loss: 0.250159\n",
      "Epoch: 2 [3200 / 60000 (5%)] \t Train Loss: 0.006898\n",
      "Epoch: 2 [3520 / 60000 (6%)] \t Train Loss: 0.023352\n",
      "Epoch: 2 [3840 / 60000 (6%)] \t Train Loss: 0.005815\n",
      "Epoch: 2 [4160 / 60000 (7%)] \t Train Loss: 0.005308\n",
      "Epoch: 2 [4480 / 60000 (7%)] \t Train Loss: 0.006386\n",
      "Epoch: 2 [4800 / 60000 (8%)] \t Train Loss: 0.010269\n",
      "Epoch: 2 [5120 / 60000 (9%)] \t Train Loss: 0.219745\n",
      "Epoch: 2 [5440 / 60000 (9%)] \t Train Loss: 0.010818\n",
      "Epoch: 2 [5760 / 60000 (10%)] \t Train Loss: 0.005995\n",
      "Epoch: 2 [6080 / 60000 (10%)] \t Train Loss: 0.029506\n",
      "Epoch: 2 [6400 / 60000 (11%)] \t Train Loss: 0.014462\n",
      "Epoch: 2 [6720 / 60000 (11%)] \t Train Loss: 0.263822\n",
      "Epoch: 2 [7040 / 60000 (12%)] \t Train Loss: 0.232214\n",
      "Epoch: 2 [7360 / 60000 (12%)] \t Train Loss: 0.117277\n",
      "Epoch: 2 [7680 / 60000 (13%)] \t Train Loss: 0.108590\n",
      "Epoch: 2 [8000 / 60000 (13%)] \t Train Loss: 0.001098\n",
      "Epoch: 2 [8320 / 60000 (14%)] \t Train Loss: 0.045569\n",
      "Epoch: 2 [8640 / 60000 (14%)] \t Train Loss: 0.107073\n",
      "Epoch: 2 [8960 / 60000 (15%)] \t Train Loss: 0.070502\n",
      "Epoch: 2 [9280 / 60000 (15%)] \t Train Loss: 0.041049\n",
      "Epoch: 2 [9600 / 60000 (16%)] \t Train Loss: 0.209847\n",
      "Epoch: 2 [9920 / 60000 (17%)] \t Train Loss: 0.226715\n",
      "Epoch: 2 [10240 / 60000 (17%)] \t Train Loss: 0.133746\n",
      "Epoch: 2 [10560 / 60000 (18%)] \t Train Loss: 0.168621\n",
      "Epoch: 2 [10880 / 60000 (18%)] \t Train Loss: 0.208096\n",
      "Epoch: 2 [11200 / 60000 (19%)] \t Train Loss: 0.002257\n",
      "Epoch: 2 [11520 / 60000 (19%)] \t Train Loss: 0.027765\n",
      "Epoch: 2 [11840 / 60000 (20%)] \t Train Loss: 0.054458\n",
      "Epoch: 2 [12160 / 60000 (20%)] \t Train Loss: 0.036026\n",
      "Epoch: 2 [12480 / 60000 (21%)] \t Train Loss: 0.013158\n",
      "Epoch: 2 [12800 / 60000 (21%)] \t Train Loss: 0.003728\n",
      "Epoch: 2 [13120 / 60000 (22%)] \t Train Loss: 0.190970\n",
      "Epoch: 2 [13440 / 60000 (22%)] \t Train Loss: 0.019041\n",
      "Epoch: 2 [13760 / 60000 (23%)] \t Train Loss: 0.157861\n",
      "Epoch: 2 [14080 / 60000 (23%)] \t Train Loss: 0.103076\n",
      "Epoch: 2 [14400 / 60000 (24%)] \t Train Loss: 0.090712\n",
      "Epoch: 2 [14720 / 60000 (25%)] \t Train Loss: 0.014416\n",
      "Epoch: 2 [15040 / 60000 (25%)] \t Train Loss: 0.017438\n",
      "Epoch: 2 [15360 / 60000 (26%)] \t Train Loss: 0.024825\n",
      "Epoch: 2 [15680 / 60000 (26%)] \t Train Loss: 0.013578\n",
      "Epoch: 2 [16000 / 60000 (27%)] \t Train Loss: 0.002139\n",
      "Epoch: 2 [16320 / 60000 (27%)] \t Train Loss: 0.004222\n",
      "Epoch: 2 [16640 / 60000 (28%)] \t Train Loss: 0.044188\n",
      "Epoch: 2 [16960 / 60000 (28%)] \t Train Loss: 0.138390\n",
      "Epoch: 2 [17280 / 60000 (29%)] \t Train Loss: 0.130255\n",
      "Epoch: 2 [17600 / 60000 (29%)] \t Train Loss: 0.098967\n",
      "Epoch: 2 [17920 / 60000 (30%)] \t Train Loss: 0.015207\n",
      "Epoch: 2 [18240 / 60000 (30%)] \t Train Loss: 0.026037\n",
      "Epoch: 2 [18560 / 60000 (31%)] \t Train Loss: 0.011436\n",
      "Epoch: 2 [18880 / 60000 (31%)] \t Train Loss: 0.003291\n",
      "Epoch: 2 [19200 / 60000 (32%)] \t Train Loss: 0.239362\n",
      "Epoch: 2 [19520 / 60000 (33%)] \t Train Loss: 0.009789\n",
      "Epoch: 2 [19840 / 60000 (33%)] \t Train Loss: 0.034682\n",
      "Epoch: 2 [20160 / 60000 (34%)] \t Train Loss: 0.024602\n",
      "Epoch: 2 [20480 / 60000 (34%)] \t Train Loss: 0.004193\n",
      "Epoch: 2 [20800 / 60000 (35%)] \t Train Loss: 0.002111\n",
      "Epoch: 2 [21120 / 60000 (35%)] \t Train Loss: 0.005741\n",
      "Epoch: 2 [21440 / 60000 (36%)] \t Train Loss: 0.214643\n",
      "Epoch: 2 [21760 / 60000 (36%)] \t Train Loss: 0.007164\n",
      "Epoch: 2 [22080 / 60000 (37%)] \t Train Loss: 0.006180\n",
      "Epoch: 2 [22400 / 60000 (37%)] \t Train Loss: 0.001112\n",
      "Epoch: 2 [22720 / 60000 (38%)] \t Train Loss: 0.012901\n",
      "Epoch: 2 [23040 / 60000 (38%)] \t Train Loss: 0.046988\n",
      "Epoch: 2 [23360 / 60000 (39%)] \t Train Loss: 0.002477\n",
      "Epoch: 2 [23680 / 60000 (39%)] \t Train Loss: 0.054952\n",
      "Epoch: 2 [24000 / 60000 (40%)] \t Train Loss: 0.125856\n",
      "Epoch: 2 [24320 / 60000 (41%)] \t Train Loss: 0.054044\n",
      "Epoch: 2 [24640 / 60000 (41%)] \t Train Loss: 0.076677\n",
      "Epoch: 2 [24960 / 60000 (42%)] \t Train Loss: 0.021496\n",
      "Epoch: 2 [25280 / 60000 (42%)] \t Train Loss: 0.164107\n",
      "Epoch: 2 [25600 / 60000 (43%)] \t Train Loss: 0.002772\n",
      "Epoch: 2 [25920 / 60000 (43%)] \t Train Loss: 0.003395\n",
      "Epoch: 2 [26240 / 60000 (44%)] \t Train Loss: 0.002682\n",
      "Epoch: 2 [26560 / 60000 (44%)] \t Train Loss: 0.046110\n",
      "Epoch: 2 [26880 / 60000 (45%)] \t Train Loss: 0.015030\n",
      "Epoch: 2 [27200 / 60000 (45%)] \t Train Loss: 0.050579\n",
      "Epoch: 2 [27520 / 60000 (46%)] \t Train Loss: 0.203919\n",
      "Epoch: 2 [27840 / 60000 (46%)] \t Train Loss: 0.012726\n",
      "Epoch: 2 [28160 / 60000 (47%)] \t Train Loss: 0.051938\n",
      "Epoch: 2 [28480 / 60000 (47%)] \t Train Loss: 0.015066\n",
      "Epoch: 2 [28800 / 60000 (48%)] \t Train Loss: 0.045975\n",
      "Epoch: 2 [29120 / 60000 (49%)] \t Train Loss: 0.001089\n",
      "Epoch: 2 [29440 / 60000 (49%)] \t Train Loss: 0.006485\n",
      "Epoch: 2 [29760 / 60000 (50%)] \t Train Loss: 0.054596\n",
      "Epoch: 2 [30080 / 60000 (50%)] \t Train Loss: 0.022453\n",
      "Epoch: 2 [30400 / 60000 (51%)] \t Train Loss: 0.165282\n",
      "Epoch: 2 [30720 / 60000 (51%)] \t Train Loss: 0.301387\n",
      "Epoch: 2 [31040 / 60000 (52%)] \t Train Loss: 0.007464\n",
      "Epoch: 2 [31360 / 60000 (52%)] \t Train Loss: 0.018961\n",
      "Epoch: 2 [31680 / 60000 (53%)] \t Train Loss: 0.053659\n",
      "Epoch: 2 [32000 / 60000 (53%)] \t Train Loss: 0.013496\n",
      "Epoch: 2 [32320 / 60000 (54%)] \t Train Loss: 0.097673\n",
      "Epoch: 2 [32640 / 60000 (54%)] \t Train Loss: 0.055653\n",
      "Epoch: 2 [32960 / 60000 (55%)] \t Train Loss: 0.002630\n",
      "Epoch: 2 [33280 / 60000 (55%)] \t Train Loss: 0.138862\n",
      "Epoch: 2 [33600 / 60000 (56%)] \t Train Loss: 0.023891\n",
      "Epoch: 2 [33920 / 60000 (57%)] \t Train Loss: 0.024804\n",
      "Epoch: 2 [34240 / 60000 (57%)] \t Train Loss: 0.348059\n",
      "Epoch: 2 [34560 / 60000 (58%)] \t Train Loss: 0.073194\n",
      "Epoch: 2 [34880 / 60000 (58%)] \t Train Loss: 0.017522\n",
      "Epoch: 2 [35200 / 60000 (59%)] \t Train Loss: 0.013687\n",
      "Epoch: 2 [35520 / 60000 (59%)] \t Train Loss: 0.022063\n",
      "Epoch: 2 [35840 / 60000 (60%)] \t Train Loss: 0.010325\n",
      "Epoch: 2 [36160 / 60000 (60%)] \t Train Loss: 0.001377\n",
      "Epoch: 2 [36480 / 60000 (61%)] \t Train Loss: 0.002435\n",
      "Epoch: 2 [36800 / 60000 (61%)] \t Train Loss: 0.013467\n",
      "Epoch: 2 [37120 / 60000 (62%)] \t Train Loss: 0.006859\n",
      "Epoch: 2 [37440 / 60000 (62%)] \t Train Loss: 0.030147\n",
      "Epoch: 2 [37760 / 60000 (63%)] \t Train Loss: 0.039078\n",
      "Epoch: 2 [38080 / 60000 (63%)] \t Train Loss: 0.013429\n",
      "Epoch: 2 [38400 / 60000 (64%)] \t Train Loss: 0.014129\n",
      "Epoch: 2 [38720 / 60000 (65%)] \t Train Loss: 0.013215\n",
      "Epoch: 2 [39040 / 60000 (65%)] \t Train Loss: 0.001319\n",
      "Epoch: 2 [39360 / 60000 (66%)] \t Train Loss: 0.016798\n",
      "Epoch: 2 [39680 / 60000 (66%)] \t Train Loss: 0.003400\n",
      "Epoch: 2 [40000 / 60000 (67%)] \t Train Loss: 0.277561\n",
      "Epoch: 2 [40320 / 60000 (67%)] \t Train Loss: 0.113523\n",
      "Epoch: 2 [40640 / 60000 (68%)] \t Train Loss: 0.004272\n",
      "Epoch: 2 [40960 / 60000 (68%)] \t Train Loss: 0.021394\n",
      "Epoch: 2 [41280 / 60000 (69%)] \t Train Loss: 0.032038\n",
      "Epoch: 2 [41600 / 60000 (69%)] \t Train Loss: 0.020605\n",
      "Epoch: 2 [41920 / 60000 (70%)] \t Train Loss: 0.029690\n",
      "Epoch: 2 [42240 / 60000 (70%)] \t Train Loss: 0.007337\n",
      "Epoch: 2 [42560 / 60000 (71%)] \t Train Loss: 0.008371\n",
      "Epoch: 2 [42880 / 60000 (71%)] \t Train Loss: 0.002331\n",
      "Epoch: 2 [43200 / 60000 (72%)] \t Train Loss: 0.005518\n",
      "Epoch: 2 [43520 / 60000 (73%)] \t Train Loss: 0.003761\n",
      "Epoch: 2 [43840 / 60000 (73%)] \t Train Loss: 0.007168\n",
      "Epoch: 2 [44160 / 60000 (74%)] \t Train Loss: 0.028621\n",
      "Epoch: 2 [44480 / 60000 (74%)] \t Train Loss: 0.000293\n",
      "Epoch: 2 [44800 / 60000 (75%)] \t Train Loss: 0.004678\n",
      "Epoch: 2 [45120 / 60000 (75%)] \t Train Loss: 0.142222\n",
      "Epoch: 2 [45440 / 60000 (76%)] \t Train Loss: 0.003783\n",
      "Epoch: 2 [45760 / 60000 (76%)] \t Train Loss: 0.228012\n",
      "Epoch: 2 [46080 / 60000 (77%)] \t Train Loss: 0.010893\n",
      "Epoch: 2 [46400 / 60000 (77%)] \t Train Loss: 0.105229\n",
      "Epoch: 2 [46720 / 60000 (78%)] \t Train Loss: 0.043020\n",
      "Epoch: 2 [47040 / 60000 (78%)] \t Train Loss: 0.012053\n",
      "Epoch: 2 [47360 / 60000 (79%)] \t Train Loss: 0.071746\n",
      "Epoch: 2 [47680 / 60000 (79%)] \t Train Loss: 0.279984\n",
      "Epoch: 2 [48000 / 60000 (80%)] \t Train Loss: 0.143284\n",
      "Epoch: 2 [48320 / 60000 (81%)] \t Train Loss: 0.003577\n",
      "Epoch: 2 [48640 / 60000 (81%)] \t Train Loss: 0.008580\n",
      "Epoch: 2 [48960 / 60000 (82%)] \t Train Loss: 0.004089\n",
      "Epoch: 2 [49280 / 60000 (82%)] \t Train Loss: 0.021876\n",
      "Epoch: 2 [49600 / 60000 (83%)] \t Train Loss: 0.017517\n",
      "Epoch: 2 [49920 / 60000 (83%)] \t Train Loss: 0.016381\n",
      "Epoch: 2 [50240 / 60000 (84%)] \t Train Loss: 0.078187\n",
      "Epoch: 2 [50560 / 60000 (84%)] \t Train Loss: 0.017260\n",
      "Epoch: 2 [50880 / 60000 (85%)] \t Train Loss: 0.120937\n",
      "Epoch: 2 [51200 / 60000 (85%)] \t Train Loss: 0.028036\n",
      "Epoch: 2 [51520 / 60000 (86%)] \t Train Loss: 0.029279\n",
      "Epoch: 2 [51840 / 60000 (86%)] \t Train Loss: 0.016131\n",
      "Epoch: 2 [52160 / 60000 (87%)] \t Train Loss: 0.024129\n",
      "Epoch: 2 [52480 / 60000 (87%)] \t Train Loss: 0.007435\n",
      "Epoch: 2 [52800 / 60000 (88%)] \t Train Loss: 0.006967\n",
      "Epoch: 2 [53120 / 60000 (89%)] \t Train Loss: 0.039253\n",
      "Epoch: 2 [53440 / 60000 (89%)] \t Train Loss: 0.062177\n",
      "Epoch: 2 [53760 / 60000 (90%)] \t Train Loss: 0.009553\n",
      "Epoch: 2 [54080 / 60000 (90%)] \t Train Loss: 0.050278\n",
      "Epoch: 2 [54400 / 60000 (91%)] \t Train Loss: 0.137025\n",
      "Epoch: 2 [54720 / 60000 (91%)] \t Train Loss: 0.005494\n",
      "Epoch: 2 [55040 / 60000 (92%)] \t Train Loss: 0.006638\n",
      "Epoch: 2 [55360 / 60000 (92%)] \t Train Loss: 0.012772\n",
      "Epoch: 2 [55680 / 60000 (93%)] \t Train Loss: 0.066903\n",
      "Epoch: 2 [56000 / 60000 (93%)] \t Train Loss: 0.003379\n",
      "Epoch: 2 [56320 / 60000 (94%)] \t Train Loss: 0.051170\n",
      "Epoch: 2 [56640 / 60000 (94%)] \t Train Loss: 0.007066\n",
      "Epoch: 2 [56960 / 60000 (95%)] \t Train Loss: 0.001519\n",
      "Epoch: 2 [57280 / 60000 (95%)] \t Train Loss: 0.058661\n",
      "Epoch: 2 [57600 / 60000 (96%)] \t Train Loss: 0.022147\n",
      "Epoch: 2 [57920 / 60000 (97%)] \t Train Loss: 0.050605\n",
      "Epoch: 2 [58240 / 60000 (97%)] \t Train Loss: 0.000939\n",
      "Epoch: 2 [58560 / 60000 (98%)] \t Train Loss: 0.002972\n",
      "Epoch: 2 [58880 / 60000 (98%)] \t Train Loss: 0.059096\n",
      "Epoch: 2 [59200 / 60000 (99%)] \t Train Loss: 0.012259\n",
      "Epoch: 2 [59520 / 60000 (99%)] \t Train Loss: 0.003642\n",
      "Epoch: 2 [59840 / 60000 (100%)] \t Train Loss: 0.037139\n",
      "\n",
      "Test Dataset\n",
      "Loss: 0.0387, Accuracy: 9868/10000 (99%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f423f56bb90>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap=\"gray\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction: 7\n",
      "Ground Truth: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Prediction: {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground Truth: {sample_targets[0]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cc446d1fc0675b147894b1dea921f56f8541700ada5bc6511fc771a50f38004"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('psd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
